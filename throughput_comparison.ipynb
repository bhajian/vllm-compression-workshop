{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cd3cc9",
   "metadata": {},
   "source": [
    "# Llama 8B Throughput: vLLM vs TensorRT-LLM\n",
    "\n",
    "Benchmark a full-precision Llama Instruct 8B model on a Linux GPU box. The notebook builds a TensorRT-LLM engine from a HF checkpoint, runs inference with TensorRT-LLM, runs the same prompts with vLLM, and compares tokens/sec. Adjust model and batch sizes for your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763cedb",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- NVIDIA GPU with CUDA 12.x drivers and enough VRAM for an 8B model in FP16/BF16.\n",
    "- Python 3.10+ in a writable env (e.g., `python -m venv .venv && source .venv/bin/activate`).\n",
    "- Optional: export `HUGGINGFACE_HUB_TOKEN` if the model is gated.\n",
    "- Make sure `nvidia-smi` shows your GPU and CUDA version ≥ 12.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (GPU drivers/CUDA 12.x assumed)\n",
    "!pip install -q --upgrade \\\n",
    "  'vllm>=0.5.5' \\\n",
    "  'tensorrt-llm-cu12' \\\n",
    "  'transformers>=4.43' \\\n",
    "  'accelerate' \\\n",
    "  'datasets' \\\n",
    "  'polygraphy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e0d69",
   "metadata": {},
   "source": [
    "## Configure benchmark\n",
    "Tweak these for your box. Increase `NUM_PROMPTS` or `MAX_NEW_TOKENS` to stress throughput, but watch VRAM. TensorRT-LLM engine is built into `./trtllm_engine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ENGINE_DIR = Path(\"trtllm_engine\")\n",
    "RESULTS_FILE = Path(\"throughput_results.json\")\n",
    "\n",
    "PROMPTS = [\n",
    "    \"Explain the benefits of quantization for LLM inference.\",\n",
    "    \"Give three bullet points on how to optimize GPU memory usage.\",\n",
    "    \"Write a short summary of the HTTP/2 protocol.\",\n",
    "    \"Provide two examples of vector databases and their use cases.\",\n",
    "]\n",
    "NUM_PROMPTS = len(PROMPTS)\n",
    "MAX_NEW_TOKENS = 128\n",
    "WARMUP_ROUNDS = 1\n",
    "MEASURE_ROUNDS = 3\n",
    "TP_SIZE = 1  # set >1 if you want tensor parallel across multiple GPUs\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Prompts: {NUM_PROMPTS}, max_new_tokens={MAX_NEW_TOKENS}, warmup={WARMUP_ROUNDS}, measure={MEASURE_ROUNDS}, tp_size={TP_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f51d2",
   "metadata": {},
   "source": [
    "## Build TensorRT-LLM engine\n",
    "This uses `trtllm-build` to convert the HF checkpoint into a TensorRT engine directory. Adjust precision flags for your GPU.\n",
    "\n",
    "- For BF16: `--enable_bf16` (Ampere+)\n",
    "- For FP16: `--enable_fp16` (Turing+)\n",
    "\n",
    "Run once; skip if you already have an engine in `ENGINE_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d245d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGINE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Example build command (BF16). Uncomment to run.\n",
    "# !trtllm-build \\\n",
    "#   --checkpoint {MODEL_ID} \\\n",
    "#   --output_dir {ENGINE_DIR} \\\n",
    "#   --max_batch_size {NUM_PROMPTS} \\\n",
    "#   --max_input_len 2048 \\\n",
    "#   --max_output_len {MAX_NEW_TOKENS} \\\n",
    "#   --gemm_plugin auto \\\n",
    "#   --tp_size {TP_SIZE} \\\n",
    "#   --enable_bf16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da03421",
   "metadata": {},
   "source": [
    "## Benchmark with TensorRT-LLM\n",
    "Uses the Python API to load the built engine and run generation. Tokens/sec counts both prompt and generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from statistics import mean\n",
    "from tensorrt_llm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=MAX_NEW_TOKENS,\n",
    ")\n",
    "\n",
    "trt_llm = LLM(\n",
    "    engine_dir=str(ENGINE_DIR),\n",
    "    tokenizer=MODEL_ID,\n",
    "    tensor_parallel_size=TP_SIZE,\n",
    ")\n",
    "\n",
    "# Warmup\n",
    "for _ in range(WARMUP_ROUNDS):\n",
    "    _ = trt_llm.generate(PROMPTS, sampling_params)\n",
    "\n",
    "trt_tps_runs = []\n",
    "for _ in range(MEASURE_ROUNDS):\n",
    "    start = time.perf_counter()\n",
    "    outputs = trt_llm.generate(PROMPTS, sampling_params)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    total_tokens = sum(len(o.prompt_token_ids) + len(o.outputs[0].token_ids) for o in outputs)\n",
    "    tps = total_tokens / elapsed\n",
    "    trt_tps_runs.append(tps)\n",
    "    print(f\"TensorRT-LLM run tokens: {total_tokens}, time: {elapsed:.3f}s, tokens/sec: {tps:.2f}\")\n",
    "\n",
    "trt_tokens_per_sec = mean(trt_tps_runs)\n",
    "print(f\"TensorRT-LLM avg tokens/sec: {trt_tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f45dbd",
   "metadata": {},
   "source": [
    "## Benchmark with vLLM\n",
    "Runs the same prompts through vLLM’s Python API for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e172ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM as VLLM, SamplingParams as VLLMSamplingParams\n",
    "\n",
    "vllm_sampling = VLLMSamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=MAX_NEW_TOKENS,\n",
    ")\n",
    "\n",
    "vllm = VLLM(\n",
    "    model=MODEL_ID,\n",
    "    tensor_parallel_size=TP_SIZE,\n",
    ")\n",
    "\n",
    "# Warmup\n",
    "for _ in range(WARMUP_ROUNDS):\n",
    "    _ = vllm.generate(PROMPTS, vllm_sampling)\n",
    "\n",
    "vllm_tps_runs = []\n",
    "for _ in range(MEASURE_ROUNDS):\n",
    "    start = time.perf_counter()\n",
    "    outputs = vllm.generate(PROMPTS, vllm_sampling)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    total_tokens = sum(len(o.prompt_token_ids) + len(o.outputs[0].token_ids) for o in outputs)\n",
    "    tps = total_tokens / elapsed\n",
    "    vllm_tps_runs.append(tps)\n",
    "    print(f\"vLLM run tokens: {total_tokens}, time: {elapsed:.3f}s, tokens/sec: {tps:.2f}\")\n",
    "\n",
    "vllm_tokens_per_sec = mean(vllm_tps_runs)\n",
    "print(f\"vLLM avg tokens/sec: {vllm_tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a8a0e",
   "metadata": {},
   "source": [
    "## Compare and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "    \"num_prompts\": NUM_PROMPTS,\n",
    "    \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "    \"measure_rounds\": MEASURE_ROUNDS,\n",
    "    \"tp_size\": TP_SIZE,\n",
    "    \"tensorrt_llm_tokens_per_sec\": trt_tokens_per_sec,\n",
    "    \"vllm_tokens_per_sec\": vllm_tokens_per_sec,\n",
    "}\n",
    "\n",
    "print(json.dumps(results, indent=2))\n",
    "RESULTS_FILE.write_text(json.dumps(results, indent=2))\n",
    "print(f\"Saved to {RESULTS_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd350737",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Keep prompts identical between runs for fairness.\n",
    "- If the TensorRT-LLM build fails, check CUDA version and GPU compatibility; switch to `--enable_fp16` if BF16 is unavailable.\n",
    "- If out-of-memory occurs, reduce `MAX_NEW_TOKENS` or batch size, or use tensor parallelism across GPUs.\n",
    "- For larger models or stricter performance, tune TensorRT-LLM build flags (e.g., KV cache size, pipeline parallelism)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
